import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
# import seaborn as sns  # ä¸éœ€è¦seaborn
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))
from datacollapse.datacollapse import fit_data_collapse, collapse_transform

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡[[memory:5669012]]
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def systematic_initial_value_analysis():
    """ç³»ç»Ÿæ€§åˆ†æä¸åŒåˆå§‹å€¼å¯¹Î½^(-1)æ‹Ÿåˆç»“æœçš„å½±å“"""
    
    print("ğŸ”¬ ç³»ç»Ÿæ€§åˆå§‹å€¼åˆ†æï¼šå…¨é¢æ¢ç´¢Î½^(-1)çš„å¯èƒ½å–å€¼")
    print("ç›®æ ‡ï¼šæµ‹è¯•å¤§é‡åˆå§‹å€¼ï¼Œåˆ†æç»“æœåˆ†å¸ƒï¼Œæ‰¾åˆ°æœ€å¯é çš„è§£")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    df_full = pd.read_csv(os.path.join(os.path.dirname(__file__), "real_data_combined.csv"))
    
    datasets = {
        'All L': df_full.copy(),
        'Drop L=7': df_full[df_full["L"] != 7].copy().reset_index(drop=True),
        'Drop L=7,9': df_full[~df_full["L"].isin([7, 9])].copy().reset_index(drop=True)
    }
    
    results = {}
    
    for dataset_name, df in datasets.items():
        print(f"\n=== åˆ†ææ•°æ®é›†: {dataset_name} ===")
        data = df[["L","U","Y"]].to_numpy(float)
        err = df["sigma"].to_numpy(float)
        
        print(f"æ•°æ®: {len(df)}ç‚¹, L={sorted(df['L'].unique())}")
        
        # 1. ç³»ç»Ÿæ€§ç½‘æ ¼æœç´¢åˆå§‹å€¼
        print("1. ç½‘æ ¼æœç´¢åˆå§‹å€¼ç©ºé—´...")
        
        # åˆ›å»ºåˆå§‹å€¼ç½‘æ ¼
        Uc_range = np.linspace(8.50, 8.80, 10)  # 10ä¸ªUcåˆå§‹å€¼
        a_range = np.linspace(0.7, 1.8, 15)     # 15ä¸ªaåˆå§‹å€¼
        
        grid_results = []
        success_count = 0
        total_count = len(Uc_range) * len(a_range)
        
        print(f"   æµ‹è¯• {total_count} ä¸ªåˆå§‹å€¼ç»„åˆ...")
        
        for i, Uc0 in enumerate(Uc_range):
            for j, a0 in enumerate(a_range):
                try:
                    # è®¾ç½®å®½æ¾çš„è¾¹ç•Œ
                    bounds = ((8.0, 9.0), (0.5, 2.5))
                    
                    (params, errs) = fit_data_collapse(data, err, Uc0, a0, 
                                                     n_knots=10, lam=1e-3, n_boot=3,
                                                     bounds=bounds)
                    
                    # è®¡ç®—åç¼©è´¨é‡
                    x_collapsed, Y_collapsed = collapse_transform(data, params)
                    x_range = x_collapsed.max() - x_collapsed.min()
                    y_ranges = []
                    for L in sorted(df["L"].unique()):
                        m = (df["L"]==L).to_numpy()
                        y_range = Y_collapsed[m].max() - Y_collapsed[m].min()
                        y_ranges.append(y_range)
                    collapse_quality = x_range / np.mean(y_ranges)
                    
                    grid_results.append({
                        'Uc0': Uc0,
                        'a0': a0,
                        'Uc_final': params[0],
                        'a_final': params[1],
                        'Uc_err': errs[0],
                        'a_err': errs[1],
                        'quality': collapse_quality,
                        'converged': True
                    })
                    success_count += 1
                    
                except Exception as e:
                    grid_results.append({
                        'Uc0': Uc0,
                        'a0': a0,
                        'Uc_final': np.nan,
                        'a_final': np.nan,
                        'Uc_err': np.nan,
                        'a_err': np.nan,
                        'quality': np.nan,
                        'converged': False
                    })
        
        grid_df = pd.DataFrame(grid_results)
        converged_df = grid_df[grid_df['converged']].copy()
        
        print(f"   æˆåŠŸæ”¶æ•›: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)")
        
        if len(converged_df) > 0:
            print(f"   Î½^(-1)èŒƒå›´: {converged_df['a_final'].min():.3f} ~ {converged_df['a_final'].max():.3f}")
            print(f"   U_cèŒƒå›´: {converged_df['Uc_final'].min():.3f} ~ {converged_df['Uc_final'].max():.3f}")
            print(f"   åç¼©è´¨é‡èŒƒå›´: {converged_df['quality'].min():.1f} ~ {converged_df['quality'].max():.1f}")
        
        # 2. éšæœºé‡‡æ ·è¡¥å……åˆ†æ
        print("2. éšæœºé‡‡æ ·è¡¥å……åˆ†æ...")
        
        np.random.seed(42)
        n_random = 200
        random_results = []
        
        for _ in range(n_random):
            # éšæœºåˆå§‹å€¼
            Uc0 = np.random.uniform(8.3, 8.9)
            a0 = np.random.uniform(0.5, 2.0)
            
            try:
                bounds = ((8.0, 9.0), (0.4, 3.0))
                (params, errs) = fit_data_collapse(data, err, Uc0, a0, 
                                                 n_knots=10, lam=1e-3, n_boot=3,
                                                 bounds=bounds)
                
                # è®¡ç®—åç¼©è´¨é‡
                x_collapsed, Y_collapsed = collapse_transform(data, params)
                x_range = x_collapsed.max() - x_collapsed.min()
                y_ranges = []
                for L in sorted(df["L"].unique()):
                    m = (df["L"]==L).to_numpy()
                    y_range = Y_collapsed[m].max() - Y_collapsed[m].min()
                    y_ranges.append(y_range)
                collapse_quality = x_range / np.mean(y_ranges)
                
                random_results.append({
                    'Uc0': Uc0,
                    'a0': a0,
                    'Uc_final': params[0],
                    'a_final': params[1],
                    'Uc_err': errs[0],
                    'a_err': errs[1],
                    'quality': collapse_quality,
                    'converged': True
                })
                
            except:
                continue
        
        random_df = pd.DataFrame(random_results)
        print(f"   éšæœºé‡‡æ ·æˆåŠŸ: {len(random_df)}/{n_random} ({len(random_df)/n_random*100:.1f}%)")
        
        # 3. åˆå¹¶æ‰€æœ‰ç»“æœè¿›è¡Œåˆ†æ
        all_results = pd.concat([converged_df, random_df], ignore_index=True)
        
        if len(all_results) > 0:
            print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡ (n={len(all_results)}):")
            print(f"   Î½^(-1): å‡å€¼={all_results['a_final'].mean():.3f}, æ ‡å‡†å·®={all_results['a_final'].std():.3f}")
            print(f"   U_c: å‡å€¼={all_results['Uc_final'].mean():.3f}, æ ‡å‡†å·®={all_results['Uc_final'].std():.3f}")
            print(f"   åç¼©è´¨é‡: å‡å€¼={all_results['quality'].mean():.1f}, æ ‡å‡†å·®={all_results['quality'].std():.1f}")
            
            # 4. è´¨é‡ç­›é€‰åˆ†æ
            quality_thresholds = [50, 70, 90, 110]
            for threshold in quality_thresholds:
                high_quality = all_results[all_results['quality'] >= threshold]
                if len(high_quality) > 0:
                    print(f"\n   è´¨é‡â‰¥{threshold}çš„è§£ (n={len(high_quality)}):")
                    print(f"     Î½^(-1): {high_quality['a_final'].mean():.3f}Â±{high_quality['a_final'].std():.3f}")
                    print(f"     U_c: {high_quality['Uc_final'].mean():.3f}Â±{high_quality['Uc_final'].std():.3f}")
            
            # 5. æ‰¾åˆ°æœ€ä½³è§£
            best_idx = all_results['quality'].idxmax()
            best_result = all_results.loc[best_idx]
            
            print(f"\nğŸ† æœ€é«˜è´¨é‡è§£:")
            print(f"   U_c = {best_result['Uc_final']:.6f} Â± {best_result['Uc_err']:.6f}")
            print(f"   Î½^(-1) = {best_result['a_final']:.6f} Â± {best_result['a_err']:.6f}")
            print(f"   åç¼©è´¨é‡ = {best_result['quality']:.2f}")
            print(f"   (èµ·å§‹å€¼: Uc0={best_result['Uc0']:.3f}, a0={best_result['a0']:.3f})")
        
        results[dataset_name] = {
            'all_results': all_results,
            'best_result': best_result if len(all_results) > 0 else None,
            'grid_df': grid_df,
            'random_df': random_df
        }
    
    return results

def create_comprehensive_visualization(results):
    """åˆ›å»ºå…¨é¢çš„å¯è§†åŒ–å›¾è¡¨"""
    
    print(f"\nğŸ¨ ç”Ÿæˆç»¼åˆå¯è§†åŒ–å›¾è¡¨...")
    
    # è®¾ç½®é¢œè‰²æ–¹æ¡ˆ
    colors = ['#2E86C1', '#E74C3C', '#F39C12']
    dataset_names = ['All L', 'Drop L=7', 'Drop L=7,9']
    
    # åˆ›å»ºå¤§å‹å›¾è¡¨
    fig = plt.figure(figsize=(20, 24))
    gs = GridSpec(6, 4, figure=fig, hspace=0.3, wspace=0.3)
    
    # 1. å‚æ•°åˆ†å¸ƒç›´æ–¹å›¾
    for i, dataset_name in enumerate(dataset_names):
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
            
        all_results = results[dataset_name]['all_results']
        
        # Î½^(-1)åˆ†å¸ƒ
        ax1 = fig.add_subplot(gs[0, i])
        ax1.hist(all_results['a_final'], bins=30, alpha=0.7, color=colors[i], edgecolor='black')
        ax1.axvline(1.0, color='red', linestyle='--', alpha=0.8, label='Î½^(-1)=1')
        ax1.set_xlabel('Î½^(-1)')
        ax1.set_ylabel('Frequency')
        ax1.set_title(f'{dataset_name}\nÎ½^(-1) Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # U_cåˆ†å¸ƒ
        ax2 = fig.add_subplot(gs[1, i])
        ax2.hist(all_results['Uc_final'], bins=30, alpha=0.7, color=colors[i], edgecolor='black')
        ax2.set_xlabel('U_c')
        ax2.set_ylabel('Frequency')
        ax2.set_title(f'U_c Distribution')
        ax2.grid(True, alpha=0.3)
    
    # 2. å‚æ•°ç›¸å…³æ€§æ•£ç‚¹å›¾
    ax3 = fig.add_subplot(gs[0, 3])
    for i, dataset_name in enumerate(dataset_names):
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
        all_results = results[dataset_name]['all_results']
        ax3.scatter(all_results['Uc_final'], all_results['a_final'], 
                   alpha=0.6, c=colors[i], label=dataset_name, s=20)
    ax3.set_xlabel('U_c')
    ax3.set_ylabel('Î½^(-1)')
    ax3.set_title('Parameter Correlation')
    ax3.axhline(1.0, color='red', linestyle='--', alpha=0.8)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 3. è´¨é‡åˆ†å¸ƒ
    ax4 = fig.add_subplot(gs[1, 3])
    for i, dataset_name in enumerate(dataset_names):
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
        all_results = results[dataset_name]['all_results']
        ax4.hist(all_results['quality'], bins=20, alpha=0.6, 
                color=colors[i], label=dataset_name, edgecolor='black')
    ax4.set_xlabel('Collapse Quality')
    ax4.set_ylabel('Frequency')
    ax4.set_title('Quality Distribution')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 4. åˆå§‹å€¼vsæœ€ç»ˆå€¼çš„å…³ç³»
    for i, dataset_name in enumerate(dataset_names):
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
            
        all_results = results[dataset_name]['all_results']
        
        # åˆå§‹a vs æœ€ç»ˆa
        ax5 = fig.add_subplot(gs[2, i])
        scatter = ax5.scatter(all_results['a0'], all_results['a_final'], 
                            c=all_results['quality'], cmap='viridis', 
                            alpha=0.7, s=30)
        ax5.plot([0.5, 2.0], [0.5, 2.0], 'r--', alpha=0.5, label='y=x')
        ax5.set_xlabel('Initial aâ‚€')
        ax5.set_ylabel('Final Î½^(-1)')
        ax5.set_title(f'{dataset_name}\nInitial vs Final a')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        plt.colorbar(scatter, ax=ax5, label='Quality')
    
    # 5. è´¨é‡vså‚æ•°å…³ç³»
    ax6 = fig.add_subplot(gs[2, 3])
    for i, dataset_name in enumerate(dataset_names):
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
        all_results = results[dataset_name]['all_results']
        ax6.scatter(all_results['a_final'], all_results['quality'], 
                   alpha=0.6, c=colors[i], label=dataset_name, s=20)
    ax6.set_xlabel('Î½^(-1)')
    ax6.set_ylabel('Collapse Quality')
    ax6.set_title('Quality vs Î½^(-1)')
    ax6.axvline(1.0, color='red', linestyle='--', alpha=0.8)
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    # 6. æœ€ä½³è§£çš„æ•°æ®åç¼©å¯è§†åŒ–
    for i, dataset_name in enumerate(dataset_names):
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
            
        # å‡†å¤‡æ•°æ®
        if dataset_name == 'All L':
            df_plot = pd.read_csv(os.path.join(os.path.dirname(__file__), "real_data_combined.csv"))
        elif dataset_name == 'Drop L=7':
            df_full = pd.read_csv(os.path.join(os.path.dirname(__file__), "real_data_combined.csv"))
            df_plot = df_full[df_full["L"] != 7].copy().reset_index(drop=True)
        else:  # Drop L=7,9
            df_full = pd.read_csv(os.path.join(os.path.dirname(__file__), "real_data_combined.csv"))
            df_plot = df_full[~df_full["L"].isin([7, 9])].copy().reset_index(drop=True)
            
        data = df_plot[["L","U","Y"]].to_numpy(float)
        best_result = results[dataset_name]['best_result']
        best_params = [best_result['Uc_final'], best_result['a_final']]
        
        # åŸå§‹æ•°æ®
        ax7 = fig.add_subplot(gs[3, i])
        for L in sorted(df_plot["L"].unique()):
            m = (df_plot["L"]==L).to_numpy()
            U_vals = df_plot["U"][m].to_numpy()
            Y_vals = df_plot["Y"][m].to_numpy()
            sigma_vals = df_plot["sigma"][m].to_numpy()
            order = np.argsort(U_vals)
            U_vals, Y_vals, sigma_vals = U_vals[order], Y_vals[order], sigma_vals[order]
            ax7.errorbar(U_vals, Y_vals, yerr=sigma_vals, fmt="o-", lw=1.2, ms=3, 
                        capsize=2, label=f"L={L}", alpha=0.8)
        ax7.set_xlabel("U")
        ax7.set_ylabel("Y")
        ax7.set_title(f"{dataset_name}\nRaw Data")
        ax7.legend()
        ax7.grid(True, alpha=0.3)
        
        # åç¼©åæ•°æ®
        ax8 = fig.add_subplot(gs[4, i])
        x_collapsed, Y_collapsed = collapse_transform(data, best_params)
        for L in sorted(df_plot["L"].unique()):
            m = (df_plot["L"]==L).to_numpy()
            xs = x_collapsed[m]
            ys = Y_collapsed[m]
            ss = df_plot["sigma"][m].to_numpy()
            order = np.argsort(xs)
            xs, ys, ss = xs[order], ys[order], ss[order]
            line, = ax8.plot(xs, ys, "-", lw=1.5, alpha=0.8, label=f"L={L}")
            ax8.errorbar(xs, ys, yerr=ss, fmt="o", ms=3, capsize=2, 
                        elinewidth=1, color=line.get_color(), alpha=0.6)
        ax8.set_xlabel("(U - Uc) Ã— L^(1/Î½)")
        ax8.set_ylabel("Y")
        ax8.set_title(f"Best Collapse\nÎ½^(-1)={best_result['a_final']:.3f}, Q={best_result['quality']:.1f}")
        ax8.legend()
        ax8.grid(True, alpha=0.3)
    
    # 7. æ€»ç»“ç»Ÿè®¡è¡¨æ ¼
    ax9 = fig.add_subplot(gs[5, :])
    ax9.axis('off')
    
    # åˆ›å»ºæ€»ç»“è¡¨æ ¼
    summary_data = []
    for dataset_name in dataset_names:
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
            
        all_results = results[dataset_name]['all_results']
        best_result = results[dataset_name]['best_result']
        
        # é«˜è´¨é‡è§£çš„ç»Ÿè®¡
        high_quality = all_results[all_results['quality'] >= 80]
        
        if len(high_quality) > 0:
            summary_data.append([
                dataset_name,
                f"{len(all_results)}",
                f"{all_results['a_final'].mean():.3f}Â±{all_results['a_final'].std():.3f}",
                f"{high_quality['a_final'].mean():.3f}Â±{high_quality['a_final'].std():.3f}" if len(high_quality) > 0 else "N/A",
                f"{best_result['a_final']:.3f}Â±{best_result['a_err']:.3f}",
                f"{best_result['quality']:.1f}",
                "âœ“" if best_result['a_final'] > 1.0 else "âœ—"
            ])
    
    if summary_data:
        table = ax9.table(cellText=summary_data,
                         colLabels=['Dataset', 'N_total', 'Mean Î½^(-1)', 'High-Q Î½^(-1)', 'Best Î½^(-1)', 'Best Quality', 'Î½^(-1)>1'],
                         cellLoc='center',
                         loc='center',
                         colWidths=[0.12, 0.08, 0.15, 0.15, 0.15, 0.12, 0.08])
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 2)
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(len(summary_data) + 1):
            for j in range(7):
                if i == 0:  # è¡¨å¤´
                    table[(i, j)].set_facecolor('#4CAF50')
                    table[(i, j)].set_text_props(weight='bold', color='white')
                else:
                    if j % 2 == 0:
                        table[(i, j)].set_facecolor('#F5F5F5')
    
    ax9.set_title('Summary Statistics of Initial Value Analysis', 
                 fontsize=14, fontweight='bold', pad=20)
    
    plt.suptitle('Comprehensive Initial Value Analysis for Î½^(-1) Parameter\nSystematic Exploration of Parameter Space', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    # ä¿å­˜å›¾è¡¨
    output_path = os.path.join(os.path.dirname(__file__), "comprehensive_initial_value_analysis.png")
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"âœ… ç»¼åˆåˆ†æå›¾è¡¨å·²ä¿å­˜: comprehensive_initial_value_analysis.png")
    return output_path

def generate_final_recommendations(results):
    """ç”Ÿæˆæœ€ç»ˆæ¨èæ–¹æ¡ˆ"""
    
    print(f"\n" + "="*70)
    print(f"ğŸ“‹ æœ€ç»ˆæ¨èæ–¹æ¡ˆåŸºäºå¤§è§„æ¨¡åˆå§‹å€¼åˆ†æ")
    print(f"="*70)
    
    print(f"\nğŸ” åˆ†ææ€»ç»“:")
    
    for dataset_name in ['All L', 'Drop L=7', 'Drop L=7,9']:
        if dataset_name not in results or results[dataset_name]['best_result'] is None:
            continue
            
        all_results = results[dataset_name]['all_results']
        best_result = results[dataset_name]['best_result']
        
        print(f"\n--- {dataset_name} ---")
        print(f"æ€»æµ‹è¯•æ¬¡æ•°: {len(all_results)}")
        
        # åˆ†è´¨é‡å±‚æ¬¡åˆ†æ
        quality_levels = [
            (120, "å“è¶Š"),
            (100, "ä¼˜ç§€"), 
            (80, "è‰¯å¥½"),
            (60, "ä¸­ç­‰"),
            (0, "å…¨éƒ¨")
        ]
        
        for min_quality, level_name in quality_levels:
            subset = all_results[all_results['quality'] >= min_quality]
            if len(subset) > 0:
                mean_a = subset['a_final'].mean()
                std_a = subset['a_final'].std()
                mean_uc = subset['Uc_final'].mean()
                std_uc = subset['Uc_final'].std()
                
                print(f"  {level_name}è´¨é‡è§£(Qâ‰¥{min_quality}, n={len(subset)}):")
                print(f"    Î½^(-1) = {mean_a:.3f} Â± {std_a:.3f}")
                print(f"    U_c = {mean_uc:.3f} Â± {std_uc:.3f}")
                
                if min_quality >= 80:  # é«˜è´¨é‡è§£çš„é¢å¤–åˆ†æ
                    above_one = len(subset[subset['a_final'] > 1.0])
                    print(f"    Î½^(-1)>1çš„æ¯”ä¾‹: {above_one}/{len(subset)} ({above_one/len(subset)*100:.1f}%)")
                break
        
        print(f"  æœ€ä½³è§£:")
        print(f"    U_c = {best_result['Uc_final']:.6f} Â± {best_result['Uc_err']:.6f}")
        print(f"    Î½^(-1) = {best_result['a_final']:.6f} Â± {best_result['a_err']:.6f}")
        print(f"    åç¼©è´¨é‡ = {best_result['quality']:.2f}")
    
    # äº¤å‰éªŒè¯ChatGPTç»“æœ
    print(f"\nğŸ¤– ä¸ChatGPTç»“æœçš„å¯¹æ¯”:")
    chatgpt_uc = 8.670
    chatgpt_a = 1.056
    
    if 'Drop L=7' in results and results['Drop L=7']['best_result'] is not None:
        our_best = results['Drop L=7']['best_result']
        print(f"ChatGPT (Drop L=7): U_c={chatgpt_uc:.3f}, Î½^(-1)={chatgpt_a:.3f}")
        print(f"æˆ‘ä»¬æœ€ä½³ (Drop L=7): U_c={our_best['Uc_final']:.3f}, Î½^(-1)={our_best['a_final']:.3f}")
        print(f"å·®å¼‚: Î”U_c={abs(our_best['Uc_final']-chatgpt_uc):.4f}, Î”Î½^(-1)={abs(our_best['a_final']-chatgpt_a):.4f}")
        
        # æ£€æŸ¥ChatGPTçš„ç»“æœåœ¨æˆ‘ä»¬çš„åˆ†å¸ƒä¸­çš„ä½ç½®
        drop_l7_results = results['Drop L=7']['all_results']
        a_percentile = (drop_l7_results['a_final'] <= chatgpt_a).mean() * 100
        uc_percentile = (drop_l7_results['Uc_final'] <= chatgpt_uc).mean() * 100
        
        print(f"ChatGPTç»“æœåœ¨æˆ‘ä»¬åˆ†å¸ƒä¸­çš„åˆ†ä½æ•°:")
        print(f"  Î½^(-1)={chatgpt_a:.3f} ä½äºç¬¬{a_percentile:.1f}ç™¾åˆ†ä½")
        print(f"  U_c={chatgpt_uc:.3f} ä½äºç¬¬{uc_percentile:.1f}ç™¾åˆ†ä½")
    
    print(f"\nğŸ¯ æœ€ç»ˆæ¨è:")
    
    # æ‰¾åˆ°æ‰€æœ‰æ•°æ®é›†ä¸­è´¨é‡æœ€é«˜çš„è§£
    all_best_results = []
    for dataset_name in results:
        if results[dataset_name]['best_result'] is not None:
            best = results[dataset_name]['best_result'].copy()
            best['dataset'] = dataset_name
            all_best_results.append(best)
    
    if all_best_results:
        overall_best = max(all_best_results, key=lambda x: x['quality'])
        
        print(f"1. æœ€é«˜è´¨é‡è§£ ({overall_best['dataset']}):")
        print(f"   U_c = {overall_best['Uc_final']:.6f} Â± {overall_best['Uc_err']:.6f}")
        print(f"   Î½^(-1) = {overall_best['a_final']:.6f} Â± {overall_best['a_err']:.6f}")
        print(f"   åç¼©è´¨é‡ = {overall_best['quality']:.2f}")
        
        print(f"\n2. å»ºè®®çš„ä¿å®ˆä¼°è®¡ (åŸºäºé«˜è´¨é‡è§£çš„å‡å€¼):")
        high_quality_all = pd.concat([
            results[name]['all_results'][results[name]['all_results']['quality'] >= 80]
            for name in results if results[name]['best_result'] is not None
        ])
        
        if len(high_quality_all) > 0:
            conservative_uc = high_quality_all['Uc_final'].mean()
            conservative_a = high_quality_all['a_final'].mean()
            conservative_uc_std = high_quality_all['Uc_final'].std()
            conservative_a_std = high_quality_all['a_final'].std()
            
            print(f"   U_c = {conservative_uc:.6f} Â± {conservative_uc_std:.6f}")
            print(f"   Î½^(-1) = {conservative_a:.6f} Â± {conservative_a_std:.6f}")
            print(f"   (åŸºäº{len(high_quality_all)}ä¸ªé«˜è´¨é‡è§£)")
        
        print(f"\n3. å¯é æ€§è¯„ä¼°:")
        print(f"   - å‚æ•°ç¨³å®šæ€§: {'å¥½' if overall_best['a_err'] < 0.05 else 'ä¸­ç­‰'}")
        print(f"   - ç‰©ç†åˆç†æ€§: {'æ˜¯' if overall_best['a_final'] > 1.0 else 'å¦'} (Î½^(-1) > 1)")
        print(f"   - åç¼©è´¨é‡: {'å“è¶Š' if overall_best['quality'] > 120 else 'ä¼˜ç§€' if overall_best['quality'] > 100 else 'è‰¯å¥½'}")

def main():
    print("ğŸ”¬ å…¨é¢åˆå§‹å€¼åˆ†æï¼šæ¢ç´¢Î½^(-1)å‚æ•°çš„çœŸå®å–å€¼")
    print("ç­–ç•¥ï¼šå¤§è§„æ¨¡æµ‹è¯•ä¸åŒåˆå§‹å€¼ï¼Œåˆ†æç»“æœåˆ†å¸ƒï¼Œæä¾›å¯è§†åŒ–éªŒè¯")
    print("="*70)
    
    # 1. ç³»ç»Ÿæ€§åˆå§‹å€¼åˆ†æ
    results = systematic_initial_value_analysis()
    
    # 2. åˆ›å»ºç»¼åˆå¯è§†åŒ–
    create_comprehensive_visualization(results)
    
    # 3. ç”Ÿæˆæœ€ç»ˆæ¨è
    generate_final_recommendations(results)
    
    print(f"\nğŸ“Š åˆ†æå®Œæˆ!")
    print(f"âœ… å·²æµ‹è¯•æ•°ç™¾ä¸ªä¸åŒåˆå§‹å€¼ç»„åˆ")
    print(f"âœ… ç”Ÿæˆäº†è¯¦ç»†çš„å¯è§†åŒ–å›¾è¡¨ä¾›æ‚¨éªŒè¯åç¼©è´¨é‡")
    print(f"âœ… æä¾›äº†åŸºäºç»Ÿè®¡åˆ†æçš„å¯é æ¨èæ–¹æ¡ˆ")
    print(f"âœ… å›¾è¡¨æ–‡ä»¶: comprehensive_initial_value_analysis.png")

if __name__ == "__main__":
    main() 